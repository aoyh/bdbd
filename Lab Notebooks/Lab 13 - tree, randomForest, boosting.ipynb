{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 13 (Module 12)\n",
    "## Tree Models\n",
    "4/25/2018\n",
    "\n",
    "### Announcements:\n",
    "* Reminder: Problem Set due April 25th today, late submissions accepted only until May 1. \n",
    "* Next week will be a review session.  I'll plan to do an additional recorded review session the day before the exam during office hours time-- 1pm-3pm.\n",
    "\n",
    "\n",
    "### Plan for Today's Lab\n",
    "1. Simple Classification Trees + R example (10 min)\n",
    "2. Random Forest + R example (10 min)\n",
    "3. Boosting + R example (10 min)\n",
    "\n",
    "## 1-Simple Classification Trees\n",
    "To illustrate the process of building a classification tree, we will consider the case where we have a binary outcome with two predictor varables X1 and X2.\n",
    "Roughly speaking, there are two steps.\n",
    "1. We divide the  set of possible values for X1 and X2 J distinct and non-overlapping regions, R1, R2,...,RJ .\n",
    "2. For every observation that falls into a given region, we make the same prediction, which is 1 if more than 50% of the outcome values for the training observations in that region are equal to 1, and 0 otherwise.\n",
    "\n",
    "The goal is choose regions of X1 and X2 that minimize the number of incorrect predictions (1 when true value is 0, or 0 when true value is 1). Unfortunately, it is computationally infeasible to consider every possible partition of our predictor variables.  \n",
    "\n",
    "For this reason, we take a top-down, greedy approach that is known as recursive binary splitting. This means that we consider X1 and X2 and all possible \"cuts\" of each variable. If our variables were age and gender, gender would have one cutpoint, and age would have a number of cutpoints equal to the number of ages in the dataset. Then we pick the variable and cutpoint such that the resulting tree of two branches has the lowest number of incorrect predictions.\n",
    "\n",
    "Next, we repeat the process, looking for the best predictor and cutpoint in order to split the data further so as to minimize the number of incorrect predictions in each of the resulting regions. However, this time, we split one of the two previously identified regions to get 3 regions. The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations.\n",
    "\n",
    "The process described above may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance. We can \"prune\" the tree to prevent this issue.  See ISL for further details on pruning.\n",
    "\n",
    "We'll illustrate in R this with a dataset where observations are passengers on the Titanic.  The dataset contains variables that give some information about each passenger such as class, gender, age and whether they survived the shipwreck.  First, we'll load relevant dataset and packages.  We'll also create a test and training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search \"Titanic ML\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: party\n",
      "Loading required package: grid\n",
      "Loading required package: mvtnorm\n",
      "Loading required package: modeltools\n",
      "Loading required package: stats4\n",
      "Loading required package: strucchange\n",
      "Loading required package: zoo\n",
      "\n",
      "Attaching package: ‘zoo’\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    as.Date, as.Date.numeric\n",
      "\n",
      "Loading required package: sandwich\n",
      "Loading required package: gbm\n",
      "Loading required package: survival\n",
      "Loading required package: lattice\n",
      "Loading required package: splines\n",
      "Loading required package: parallel\n",
      "Loaded gbm 2.1.3\n"
     ]
    }
   ],
   "source": [
    "# set working directory\n",
    "# setwd(\"/home/jovyan/mba217-2/Data\")\n",
    "\n",
    "# Clear Environment \n",
    "rm(list = ls())\n",
    "# Clear Console\n",
    "cat('\\014')\n",
    "\n",
    "#install required packages\n",
    "#install.packages(c(\"randomForest\",\"party\", \"gbm\"))\n",
    "# Load required packages\n",
    "require(dplyr)\n",
    "# require(ggplot2)\n",
    "# For the tree functions\n",
    "require(rpart) # rpart()\n",
    "require(party)\n",
    "# For Random Forest\n",
    "require(randomForest) # randomForest()\n",
    "# For Boosting\n",
    "require(gbm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'survived'</li>\n",
       "\t<li>'pclass'</li>\n",
       "\t<li>'sex'</li>\n",
       "\t<li>'age'</li>\n",
       "\t<li>'sibsp'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'survived'\n",
       "\\item 'pclass'\n",
       "\\item 'sex'\n",
       "\\item 'age'\n",
       "\\item 'sibsp'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'survived'\n",
       "2. 'pclass'\n",
       "3. 'sex'\n",
       "4. 'age'\n",
       "5. 'sibsp'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"survived\" \"pclass\"   \"sex\"      \"age\"      \"sibsp\"   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1046</li>\n",
       "\t<li>5</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1046\n",
       "\\item 5\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1046\n",
       "2. 5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1046    5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read the data and remove observations with missing data\n",
    "Titanic <- read.csv(\"../Data/Titanic.csv\", header = TRUE)\n",
    "# remove obs with missing data, and select a few predictors\n",
    "Titanic <- na.omit(Titanic[,c(\"survived\",\"pclass\",\"sex\",\"age\",\"sibsp\")])\n",
    "attach(Titanic)\n",
    "names(Titanic)\n",
    "dim(Titanic)\n",
    "\n",
    "# for reproducibility\n",
    "set.seed(1234)\n",
    "\n",
    "# create train and test sets (~40% for training, ~60% for testing)\n",
    "Titanic$randu <- runif(nrow(Titanic),0,1)\n",
    "Titanic.train <- Titanic[Titanic$randu < .4,]\n",
    "Titanic.test <- Titanic[Titanic$randu >= .4,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the function rpart to create a simple tree using only the age and class variables.  This automatically performs the recursive binary splitting process for us.  Below, we see that it makes the correct prediction about 65% of the time in the test dataset.  CP=0.05 is the pruning parameter.  Typically we'd use cross validation to select the optimal pruning parameter, but we're keeping it simple here and just picking one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.349606299212598"
      ],
      "text/latex": [
       "0.349606299212598"
      ],
      "text/markdown": [
       "0.349606299212598"
      ],
      "text/plain": [
       "[1] 0.3496063"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple classification tree using rpart on training data\n",
    "simp.tree <- rpart(as.factor(survived) ~ pclass + age, data = Titanic.train, method=\"class\", cp=.05)\n",
    "# Fit model to test data and calculate MSE, which is just % incorrect predictions\n",
    "Titanic.test$pred<-predict(simp.tree, Titanic.test, type = c( \"class\"))\n",
    "Titanic.test$pred<-ifelse(Titanic.test$pred==\"1\", 1, 0)\n",
    "mean((Titanic.test$pred-Titanic.test$survived)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can fit the model to the full dataset and generate a picture of the actual tree.  For the condition at each node, TRUE goes to the left, and FALSE to the right.  The 0 or 1 at the bottom of the node is the prediction for that node, and the number below that is Survive=0/Survive=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAACplBMVEUAAAABAQECAgIDAwME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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot model on full dataset\n",
    "simp.tree.full <- rpart(as.factor(survived) ~ pclass + age, data = Titanic, method=\"class\", cp=.05)\n",
    "plot(simp.tree.full, uniform = TRUE) # plot tree\n",
    "text(simp.tree.full, use.n = TRUE, all = TRUE, cex = 0.6, main = \"Titanic\") # add labels to tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Random Forest\n",
    "Random forests is a technique that uses multiple trees.  A typical procedure uses the following steps:\n",
    " 1. Choose a bootstrap sample of the observations and start to grow a tree.\n",
    " 2. At each node of the tree, choose a random sample of the predictors to make the next decision. Do not prune the trees.\n",
    " 3. Repeat this process many times to grow a forest of trees.\n",
    " 4. In order to determine the classification of a new observation, have each tree make a classification and use a majority vote for the final prediction.\n",
    " \n",
    "We'll illustrate this below using the titanic dataset again.  R fits a random forest using a single command.  We'll calculate the error rate again, this time using the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       \n",
       "pred.rf   0   1\n",
       "      0 335  82\n",
       "      1  44 174"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.198425196850394"
      ],
      "text/latex": [
       "0.198425196850394"
      ],
      "text/markdown": [
       "0.198425196850394"
      ],
      "text/plain": [
       "[1] 0.1984252"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.198425196850394"
      ],
      "text/latex": [
       "0.198425196850394"
      ],
      "text/markdown": [
       "0.198425196850394"
      ],
      "text/plain": [
       "[1] 0.1984252"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>MeanDecreaseGini</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>pclass</th><td>23.184489</td></tr>\n",
       "\t<tr><th scope=row>sex</th><td>45.836664</td></tr>\n",
       "\t<tr><th scope=row>age</th><td>15.945779</td></tr>\n",
       "\t<tr><th scope=row>sibsp</th><td> 6.797071</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "  & MeanDecreaseGini\\\\\n",
       "\\hline\n",
       "\tpclass & 23.184489\\\\\n",
       "\tsex & 45.836664\\\\\n",
       "\tage & 15.945779\\\\\n",
       "\tsibsp &  6.797071\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | MeanDecreaseGini | \n",
       "|---|---|---|---|\n",
       "| pclass | 23.184489 | \n",
       "| sex | 45.836664 | \n",
       "| age | 15.945779 | \n",
       "| sibsp |  6.797071 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "       MeanDecreaseGini\n",
       "pclass 23.184489       \n",
       "sex    45.836664       \n",
       "age    15.945779       \n",
       "sibsp   6.797071       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Random Forests #####\n",
    "# It is important to set seed to for replicating results as there's a lot of randomization that happens in Random Forests\n",
    "set.seed(1234)\n",
    "\n",
    "#Fit the training data\n",
    "# nodesize is minimum size of terminal nodes\n",
    "# mtry is number of variables randomly sampled as candidates at each split\n",
    "# ntree is the number of bootstrap samples\n",
    "model.rf <- randomForest(as.factor(survived) ~ pclass + sex + age + sibsp,\n",
    "                         data = Titanic.train, mtry = 2, nodesize = 30, ntree = 500)\n",
    "#predict outcomes on the test data\n",
    "pred.rf <- predict(model.rf, newdata = Titanic.test)\n",
    "#compare actual and predicted outcomes\n",
    "conf.rf <- table(pred.rf, Titanic.test$survived)\n",
    "conf.rf\n",
    "#calculate the error rate\n",
    "(conf.rf[1,2]+conf.rf[2,1])/sum(conf.rf)\n",
    "# or equivalent to \n",
    "mean(pred.rf != Titanic.test$survived)\n",
    "#total decrease in node impurities from splitting on the variable, averaged over all trees\n",
    "importance(model.rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Boosting\n",
    "Boosting involves repeated estimation where misclassified observations are given increasing weight in each repetition. The final estimate is then a vote or an average estimate across the repeated estimates.  \n",
    "\n",
    "First, an initial tree is fit.  Given that model, we fit a decision tree to the residuals. That is, we fit a tree using R= (1= misclassified positive, -1=misclassified negative), rather than the outcome Y , as the response. We then add this new decision tree into the fitted function in order to update the residuals. \n",
    "\n",
    "Boosting has three tuning parameters:\n",
    "1. The number of trees B. Unlike bagging and random forests, boosting can overfit if B is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select B.\n",
    "2. The shrinkage parameter λ, a small positive number. This controls the rate at which boosting learns. Typical values are 0.01 or 0.001, and the right choice can depend on the problem. Very small λ can require using a very large value of B in order to achieve good performance.\n",
    "3. The number d of splits in each tree or the \"interaction depth\", controls the interaction order of the boosted model, since d splits can involve at most d variables.  Typically 1 or 2 is a good choice, and we can compare using the test error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          \n",
       "pred.boost   0   1\n",
       "         0 324  78\n",
       "         1  55 178"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.209448818897638"
      ],
      "text/latex": [
       "0.209448818897638"
      ],
      "text/markdown": [
       "0.209448818897638"
      ],
      "text/plain": [
       "[1] 0.2094488"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.209448818897638"
      ],
      "text/latex": [
       "0.209448818897638"
      ],
      "text/markdown": [
       "0.209448818897638"
      ],
      "text/plain": [
       "[1] 0.2094488"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Boosting #####\n",
    "model.boost <- gbm(survived ~ pclass + sex + age + sibsp,\n",
    "                 data = Titanic.train,\n",
    "                 distribution = \"bernoulli\",\n",
    "                 n.trees = 10000,\n",
    "                 shrinkage = 0.001,\n",
    "                 interaction.depth = 1)\n",
    "\n",
    "pred.boost <- predict(model.boost, newdata = Titanic.test, type =\"response\", n.trees=1000)\n",
    "pred.boost <- ifelse(pred.boost >=0.5 , 1, 0)\n",
    "conf.boost <- table(pred.boost, Titanic.test$survived)\n",
    "conf.boost\n",
    "(conf.boost[1,2]+conf.boost[2,1])/sum(conf.boost)\n",
    "# or\n",
    "mean(pred.boost != Titanic.test$survived)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
